{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1808010,"sourceType":"datasetVersion","datasetId":1074124},{"sourceId":3167972,"sourceType":"datasetVersion","datasetId":1796217},{"sourceId":3355768,"sourceType":"datasetVersion","datasetId":2024507},{"sourceId":6370358,"sourceType":"datasetVersion","datasetId":3057430},{"sourceId":7456318,"sourceType":"datasetVersion","datasetId":4300143},{"sourceId":7483333,"sourceType":"datasetVersion","datasetId":4340387}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:28:05.337966Z","iopub.execute_input":"2024-01-27T07:28:05.338360Z","iopub.status.idle":"2024-01-27T07:28:05.860376Z","shell.execute_reply.started":"2024-01-27T07:28:05.338329Z","shell.execute_reply":"2024-01-27T07:28:05.859530Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport torch\nfrom torch import *\ndataset_x = torch.load(\"/kaggle/input/zip-40k-dataset-cxr/final_x/final_x.pt\")\ndataset_y = torch.load(\"/kaggle/input/zip-40k-dataset-cxr/final_y (1).pt\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:28:05.861900Z","iopub.execute_input":"2024-01-27T07:28:05.862191Z","iopub.status.idle":"2024-01-27T07:29:29.636785Z","shell.execute_reply.started":"2024-01-27T07:28:05.862166Z","shell.execute_reply":"2024-01-27T07:29:29.635931Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtr, xts, ytr, yts = train_test_split(dataset_x, dataset_y, test_size=0.2, shuffle=True, stratify=dataset_y)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:29.637955Z","iopub.execute_input":"2024-01-27T07:29:29.638369Z","iopub.status.idle":"2024-01-27T07:29:32.382182Z","shell.execute_reply.started":"2024-01-27T07:29:29.638344Z","shell.execute_reply":"2024-01-27T07:29:32.381376Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"xtr.size()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.384839Z","iopub.execute_input":"2024-01-27T07:29:32.385577Z","iopub.status.idle":"2024-01-27T07:29:32.393978Z","shell.execute_reply.started":"2024-01-27T07:29:32.385540Z","shell.execute_reply":"2024-01-27T07:29:32.393184Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([32301, 1, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"xts.size()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.395051Z","iopub.execute_input":"2024-01-27T07:29:32.395325Z","iopub.status.idle":"2024-01-27T07:29:32.405560Z","shell.execute_reply.started":"2024-01-27T07:29:32.395303Z","shell.execute_reply":"2024-01-27T07:29:32.404743Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"torch.Size([8076, 1, 224, 224])"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import *\nmy_dataset = torch.utils.data.TensorDataset(xtr,ytr) # create your datset\nmy_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=32,\n                                          shuffle=True, num_workers=2)\n\nmy_dataset = torch.utils.data.TensorDataset(xts,yts) # create your datset\nmy_tdataloader = torch.utils.data.DataLoader(my_dataset,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.406748Z","iopub.execute_input":"2024-01-27T07:29:32.406977Z","iopub.status.idle":"2024-01-27T07:29:32.415868Z","shell.execute_reply.started":"2024-01-27T07:29:32.406956Z","shell.execute_reply":"2024-01-27T07:29:32.415155Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(my_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.416768Z","iopub.execute_input":"2024-01-27T07:29:32.416987Z","iopub.status.idle":"2024-01-27T07:29:32.427415Z","shell.execute_reply.started":"2024-01-27T07:29:32.416967Z","shell.execute_reply":"2024-01-27T07:29:32.426601Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1010"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.428604Z","iopub.execute_input":"2024-01-27T07:29:32.428986Z","iopub.status.idle":"2024-01-27T07:29:32.435557Z","shell.execute_reply.started":"2024-01-27T07:29:32.428955Z","shell.execute_reply":"2024-01-27T07:29:32.434749Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"rep = []","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:32.436684Z","iopub.execute_input":"2024-01-27T07:29:32.437015Z","iopub.status.idle":"2024-01-27T07:29:32.447784Z","shell.execute_reply.started":"2024-01-27T07:29:32.436984Z","shell.execute_reply":"2024-01-27T07:29:32.446960Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom tempfile import TemporaryDirectory\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, random_split\n\nimport torch\nimport torchvision.models as models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n\n\nimport timm\n\n# from timm\npretrained_model_name = \"inception_v3\"\nmodel = timm.create_model(pretrained_model_name, pretrained=True)\nprint(model)\nnum_channels = 1  # for grayscale images, but it could be any number\n# Extract the first conv layer's parameters\nnum_filters = model.Conv2d_1a_3x3.conv.out_channels\nkernel_size = model.Conv2d_1a_3x3.conv.kernel_size\nstride = model.Conv2d_1a_3x3.conv.stride\npadding = model.Conv2d_1a_3x3.conv.padding\nconv1 = torch.nn.Conv2d(num_channels, num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\noriginal_weights = model.Conv2d_1a_3x3.conv.weight.data.mean(dim=1, keepdim=True)\nconv1.weight.data = original_weights.repeat(1, num_channels, 1, 1)\nmodel.Conv2d_1a_3x3.conv = conv1\nmodel_ft = model\n\n\n# Freeze only the convolutional layers of the pre-trained model\nfor param in model_ft.parameters():\n    if isinstance(param, nn.Conv2d):\n        param.requires_grad = False\n\n\n\n# Modify the model head for fine-tuning\nnum_features = 2048\n\n# Additional linear layer and dropout layer\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 256),  # Additional linear layer with 256 output features\n    nn.ReLU(inplace=True),         # Activation function (you can choose other activation functions too)\n    nn.Dropout(0.5),               # Dropout layer with 50% probability\n    nn.Linear(256, 4),    # Final prediction fc layer\n    nn.Softmax()\n)\n\nmodel_ft = torch.nn.DataParallel(model_ft, device_ids = [0,1]).to(device)\n#model_ft = model_ft.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n\n\nbatch_size=32\nnum_epochs=30\nval_size = len(my_tdataloader)\ntrain_size = len(my_dataloader) \n\nlosses = [] \naccuracies = [] \nval_losses = [] \nval_accuracies = [] \n# Train the model \ncnt = 0\nx = range(0,num_epochs)\nfor epoch in x: \n    cnt = 0\n    for i, (images, labels) in enumerate(my_dataloader): \n        # Forward pass \n        images=images.to(device) \n        labels=labels.type(torch.LongTensor).to(device) \n        outputs = model(images) \n        loss = criterion(outputs, labels) \n        \n        # Backward pass and optimization \n        optimizer_ft.zero_grad() \n        loss.backward() \n        optimizer_ft.step() \n        cnt = cnt+1\n        print(cnt)\n        clear_output()\n        _, predicted = torch.max(outputs.data, 1)\n        \n    acc = (predicted == labels).sum().item() / labels.size(0) \n    accuracies.append(acc) \n    losses.append(loss.item()) \n\n    # Evaluate the model on the validation set\t\n    rep.append('Epoch [{}/{}],Loss:{:.4f},Accuracy:{:.2f}'.format( \n        epoch+1, num_epochs, loss.item(),acc))\n    print('Epoch [{}/{}],Loss:{:.4f},Accuracy:{:.2f}'.format( \n        epoch+1, num_epochs, loss.item(),acc))\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:33:29.286116Z","iopub.execute_input":"2024-01-27T07:33:29.286507Z","iopub.status.idle":"2024-01-27T07:33:41.268435Z","shell.execute_reply.started":"2024-01-27T07:33:29.286476Z","shell.execute_reply":"2024-01-27T07:33:41.267077Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(my_dataloader): \n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Forward pass \u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     images\u001b[38;5;241m=\u001b[39m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     89\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images) \n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"rep","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:35.674412Z","iopub.status.idle":"2024-01-27T07:29:35.674767Z","shell.execute_reply.started":"2024-01-27T07:29:35.674596Z","shell.execute_reply":"2024-01-27T07:29:35.674613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\n\ny_pred = []\ny_true = []  \n\n# iterate over test data\nfor inputs, labels in my_tdataloader:\n        output = model_ft(inputs) # Feed Network\n\n        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n        y_pred.extend(output) # Save Prediction\n        \n        labels = labels.data.cpu().numpy()\n        y_true.extend(labels) # Save Truth\n\n# constant for classes\nclasses = ('COVID','Normal','Pneumonia','Bacterial infection')\n\n# Build confusion matrix\ncf_matrix = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n                     columns = [i for i in classes])\nplt.figure(figsize = (12,7))\nsn.heatmap(df_cm, annot=True)\nplt.savefig('output.png')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:35.675759Z","iopub.status.idle":"2024-01-27T07:29:35.676204Z","shell.execute_reply.started":"2024-01-27T07:29:35.675970Z","shell.execute_reply":"2024-01-27T07:29:35.675991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred))\nclassification_report(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:29:35.678264Z","iopub.status.idle":"2024-01-27T07:29:35.678592Z","shell.execute_reply.started":"2024-01-27T07:29:35.678432Z","shell.execute_reply":"2024-01-27T07:29:35.678447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}